{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJMortensonWarwick/ADA2425/blob/main/6_X_fundamental_math_for_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fundamental Mathematics for Deep Learning"
      ],
      "metadata": {
        "id": "yAF-skjlrH0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will cover (in as gentle a fashion as possible) some of the underlying mathematical concepts underpinning deep learning. (We'll also sneak in a bit of an intro to some concepts in PyTorch, which will be the solution we use in the module). Now I'm sure that has whet your appetite enough so let's begin!"
      ],
      "metadata": {
        "id": "65_0QqsmrJv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scalars, Vectors, Matrices and Tensors\n",
        "Although we've actually seen some of these concepts already in a programming sense, its worth going over some of them from a more mathematical perspective (and also some of the operations we can apply to them). Let's start with the simplest of these, a scalar:"
      ],
      "metadata": {
        "id": "8g3dCCr5ra3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the packages we need in this tutorial\n",
        "import torch\n",
        "import numpy as np\n",
        "import sympy as sym\n",
        "\n",
        "# Example scalar in PyTorch\n",
        "my_scalar = torch.tensor(13)\n",
        "my_scalar"
      ],
      "metadata": {
        "id": "T4WNKUQzlW9R",
        "outputId": "8a892e4e-2ba3-4df7-87c1-d02f4980e473",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(13)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Really simple, a scalar is just a single numerical value we may want to use in our calculations or reporting. It can be an integer or a float or any other numerical type.\n",
        "\n",
        "A matrix (plural matrices), is a little more nuanced (but not a lot):"
      ],
      "metadata": {
        "id": "3LHjSJHjYW_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example matrix in PyTorch\n",
        "basic_matrix = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "basic_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znqRlWVPmAEt",
        "outputId": "e9e85aa6-ede4-424b-8a00-8500e0e1b3b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3, 4],\n",
              "        [5, 6, 7, 8]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our programmer's hat on we may say we have built a list of 2x lists (with the square brackets). However, by virtue of the fact each list is of the same length, we have effectively built a two-dimensional table (as we would build a DataFrame) which in this case has two rows and four columns. Often you would see this described as an $M$x$N$ matrix (where $M$ is the number of rows and $N$ the number of columns). Confusingly you'll sometimes see it described as an $N$x$M$ matrix where $N$ is rows and $M$ columns ... TL;DR its always rows by columns.\n",
        "\n",
        "Let's look at a special case of a matrix ... a vector:"
      ],
      "metadata": {
        "id": "J4ugwtkBxm2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example vector in PyTorch (a vector is a special case of a matrix)\n",
        "eg_vector = torch.tensor([1, 2, 3, 4])  # or torch.tensor([[1], [2], [3], [4]]) for a column vector\n",
        "eg_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mYLLAxEm1PR",
        "outputId": "3b9240b6-4762-4644-8872-9940804f950b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Effectively a vector is a $M$x$1$ matrix (a single column). Again, this is slightly confusing when programmed as we code it horizontally although conceptually we would consider it as a vertical slice.\n",
        "\n",
        "Our final type for this Notebook is the tensor ... which have been popularised (in ML) by deep learning and tools like TensorFlow. Actually everything we have produced so far is a tensor as we can see in the outputs:"
      ],
      "metadata": {
        "id": "fdSEG66-0Ma7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_scalar)\n",
        "print(basic_matrix)\n",
        "print(eg_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5RKVWuxzlyz",
        "outputId": "845bb7be-564e-471c-d61a-93b67881f2ea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(13)\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n",
            "tensor([1, 2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see a tensor acts as basically a container for our other numeric types. We can see \"my_scalar\" returns a tensor with no shape (basically our single value - 13); \"basic_matrix\" contains our matrix which is shape(2, 4) (2x rows, 4x columns); and \"eg_vector\" conains the vector of shape(4, 0) (4x rows, 1x column).\n",
        "\n",
        "We often refer to these different shapes as rank-$n$ tensors, such that:\n",
        "* A rank-0 tensor stores a scalar (e.g. \"my_scalar\")\n",
        "* A rank-1 tensor stores a vector\n",
        "* A rank-2 tensor stores a 2D matrix (e.g, a standard DataFrame)\n",
        "* A rank-3 tensor has three dimensions (e.g. a digital image stored in RGB format - rows, columns and a colour dimensions)\n",
        "* A rank-4 tensor adds a fourth dimension - e.g. a batch of rank-3 images.\n",
        "\n",
        "So what do we gain from putting our objects in tensors? We could go into a long discussion into what a tensor really is, from a mathematical and/or physics sense, but in practice we just care about two things:\n",
        "1. Tensors are a slightly more efficient and when we work at scale (and deep learning loves big datasets). Small efficiencies can make a big difference. In particular, compared to something like _numpy_, tensors can be used more easily with GPUs;\n",
        "2. Tensors can be more connected into a system and can change their values when other values in the system change. In deep learning, this means keeping track of gradients and computational graphs (which we'll discuss in the module)."
      ],
      "metadata": {
        "id": "x3K2sVkcaa4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Algebra\n",
        "Matrix algebra is a big topic, and we don't need to go too far down the rabbit hole. However, there are some key topics that underpin a lot of deep learning (and ML for that matter) which, while more a backend operation than a frontend (i.e. you don't typically need to do the calculations yourself), it helps understand how these algorithms work.\n",
        "\n",
        "Our first topic will be multiplying matrices. There are two main ways we can do this - _element\\-wise_ and _matrix_ multiplication.<br><br>\n",
        "\n",
        "\n",
        "### Element-wise Multiplication\n",
        "Element-wise is probably the more obvious. It depends on both elements being of the same size. Let's look at some examples (using _tf.multiply_):"
      ],
      "metadata": {
        "id": "bFgy4GHxiOgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Element-wise multiplication in PyTorch\n",
        "matrix_one = torch.tensor([1, 2, 3, 4])\n",
        "matrix_two = torch.tensor([5, 6, 7, 8])\n",
        "ew_matrix = matrix_one * matrix_two  # Element-wise multiplication in PyTorch\n",
        "print(f\"First matrix: {ew_matrix}\")\n",
        "print(\"\\n\") # blank line\n",
        "\n",
        "matrix_three = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "matrix_four = torch.tensor([[8, 7, 6, 5], [4, 3, 2, 1]])\n",
        "ew_matrix_two = matrix_three * matrix_four\n",
        "print(f\"Second matrix: {ew_matrix_two}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saFqAJpineQC",
        "outputId": "6b33c118-7ca5-45d1-c5f5-a43513f969b3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First matrix: tensor([ 5, 12, 21, 32])\n",
            "\n",
            "\n",
            "Second matrix: tensor([[ 8, 14, 18, 20],\n",
            "        [20, 18, 14,  8]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, effectively we for loop for each list and multiply each item with its item in the corresponding list. So the 1st item of the 1st list (1) is multiplied with the 1st item of the 2nd list (5) and this produces the first item of the output (5). In the two row version we effectively multiple the top left item with the bottom left item, and so on."
      ],
      "metadata": {
        "id": "7QIAld3PmsNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix by Vector Multiplication\n",
        "Although different sizes, matrix by vector multiplication is always element-wise. The size of the matrix is the size of the output:"
      ],
      "metadata": {
        "id": "Kkhm5aCbnUZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix by vector multiplication\n",
        "vector_one = torch.tensor([1, 2, 3, 4])\n",
        "matrix_four = torch.tensor([[8, 7, 6, 5], [4, 3, 2, 1]])\n",
        "ew_matrix_three = vector_one * matrix_four # Element-wise multiplication\n",
        "print(f\"Third matrix: {ew_matrix_three}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keX_HoAjoDOK",
        "outputId": "93108ffc-aec7-44a6-96af-7c18f3e62599"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Third matrix: tensor([[ 8, 14, 18, 20],\n",
            "        [ 4,  6,  6,  4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix by Scalar Multiplication\n",
        "Similarly, multiplying by a scalar is element-wise:"
      ],
      "metadata": {
        "id": "QV7oYA5SDgDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix by scalar multiplication\n",
        "another_scalar = 10\n",
        "matrix_four = torch.tensor([[8, 7, 6, 5], [4, 3, 2, 1]])\n",
        "ew_matrix_four = another_scalar * matrix_four\n",
        "ew_matrix_four"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELm6gzgWodCC",
        "outputId": "ceeb4f87-4e71-4bb4-dd76-579aed1db631"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[80, 70, 60, 50],\n",
              "        [40, 30, 20, 10]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix Multiplication\n",
        "Matrix multiplication is more flexible than element-wise in that it doesn't require the matrices to be of the same size. However, it is slightly less obvious how it works. Again, we'll look at a couple of examples (using _torch.matmul_ ... as in __mat__rix __mul__tiplication):"
      ],
      "metadata": {
        "id": "GEoHAtbEqPsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix Multiplication\n",
        "matrix_five = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
        "matrix_six = torch.tensor([[100], [200]])\n",
        "matmul_matrix = torch.matmul(matrix_five, matrix_six)  # or matrix_five @ matrix_six\n",
        "matmul_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxWW8Atfo_zp",
        "outputId": "2e5049e0-968c-417e-99ac-887c96d8b5fb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 500],\n",
              "        [1100],\n",
              "        [1700]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This may be a little less obvious so let's go through the math. Our output is a 3x row vector so lets see the maths of each row:\n",
        "* $1 \\times 100 + 2 \\times 200 = 100 + 400 = 500$\n",
        "* $3 \\times 100 + 4 \\times 200 = 300 + 800 = 1100$\n",
        "* $5 \\times 100 + 6 \\times 200 = 500 + 1200 = 1700$"
      ],
      "metadata": {
        "id": "jOHRcrdFrpTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_seven = torch.tensor([[1, 2, 3] , [4, 5, 6]])\n",
        "matrix_eight = torch.tensor([[100, 200], [300, 400], [500, 600]])\n",
        "matmul_matrix_two = torch.matmul(matrix_seven, matrix_eight)\n",
        "matmul_matrix_two"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ_g4s8bsyxN",
        "outputId": "b3303b29-c21f-43e1-f5a6-a3b71ffef20c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2200, 2800],\n",
              "        [4900, 6400]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Effectively here we have a $3\\times2$ matrix and a $2\\times3$ matrix. When we multiply the two together we effectively take the first row of the first matrix and multiply it by the first column of the second to form the first value in our output; then the first row of the first matrix by the second column of the second to form the second output, and so on. Again, in the form of our $2\\times2$ output, the maths is:<br><br>\n",
        "$1\\times100 + 2\\times300 + 3\\times500 = 100 + 600 + 1500 = 2200$\n",
        "<br>\n",
        "$1\\times200 + 2\\times400 + 3\\times600 = 200 + 800 + 1800 = 2800$\n",
        "<br>\n",
        "$4\\times100 + 5\\times300 + 6\\times500 = 400 + 1500 + 3000 = 4900$\n",
        "<br>\n",
        "$4\\times200 + 5\\times400 + 6\\times600 = 800 + 2000 + 3600 = 6400$"
      ],
      "metadata": {
        "id": "X70L-iB9tWj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Dot Product\n",
        "The dot product of two vectors is an equivalent calculation to matrix multiplication via _matmul_. However, given that we are working with vectors we end up with a single number (a scalar). For example:"
      ],
      "metadata": {
        "id": "YIKTxb76EosF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_one = torch.tensor([1, 2, 3, 4])\n",
        "vector_two = torch.tensor([8, 7, 6, 5])\n",
        "\n",
        "vector_dot_product = torch.dot(vector_one, vector_two)\n",
        "vector_dot_product"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyhEYGvrpz_y",
        "outputId": "1659e13e-b01f-4805-8b0c-0d77ce58ec4f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(60)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the math again:<br><br>\n",
        "$ 1 \\times 8 + 2 \\times 7 + 3 \\times 6 + 4 \\times 5 = 8 + 14 + 18 + 20 = 60$"
      ],
      "metadata": {
        "id": "hIU-lXVIGN9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix Addition and Reduce-Sum\n",
        "Matrix addition works as you may expect, but does rely on equal size matrices. Let's see an example again:"
      ],
      "metadata": {
        "id": "ypJg_bjrwDE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_nine = tf.constant([1, 2, 3])\n",
        "matrix_ten = tf.constant([6, 5, 4])\n",
        "matrix_addition = tf.add(matrix_nine, matrix_ten)\n",
        "matrix_addition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBMqnIbQzHv3",
        "outputId": "f3a95e54-72aa-449d-c091-a1462465f587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([7, 7, 7], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix Addition\n",
        "matrix_nine = torch.tensor([1, 2, 3])\n",
        "matrix_ten = torch.tensor([6, 5, 4])\n",
        "matrix_addition = matrix_nine + matrix_ten # Direct addition in PyTorch\n",
        "matrix_addition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoRpKPY-qBrG",
        "outputId": "b7441c34-d6b5-45b3-ca5a-0a8d3bd96efc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([7, 7, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ultimately this is just an element-wise addition. E.g.\n",
        "<br><br>\n",
        "$ 1 + 6 = 7$\n",
        "<br>\n",
        "$ 2 + 5 = 7$\n",
        "<br>\n",
        "$ 3 + 4 = 7$"
      ],
      "metadata": {
        "id": "NJRXfTsozpAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another related concept is _sum_, which is related to a MapReduce approach (later in the term). Nothing like an example amiright?"
      ],
      "metadata": {
        "id": "fZLQ-kKw0Jdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_eleven = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "matrix_sum = torch.sum(matrix_eleven)\n",
        "matrix_sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BSTdobDqXOp",
        "outputId": "51ee440e-93fc-407a-ce58-f0c3c38172af"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(45)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essentially the function has reduced our matrix to a single value - by simply summing up all of the nine values.\n",
        "\n",
        "We can also sum by columns or rows:"
      ],
      "metadata": {
        "id": "eub3K-v02BR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix Sum\n",
        "matrix_eleven = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "matrix_sr_cols = torch.sum(matrix_eleven, dim=0) # Sum along columns\n",
        "print(matrix_sr_cols)\n",
        "\n",
        "matrix_sr_rows = torch.sum(matrix_eleven, dim=1) # Sum along rows\n",
        "matrix_sr_rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fa8K3KRqql4",
        "outputId": "1d4f3f50-e313-4fe5-bb35-0c5c38cacaff"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([12, 15, 18])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 6, 15, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have an output of _shape=(3,)_ in each case. In the first (\"matrix_sr_cols\") the calculations is:\n",
        "<br><br>\n",
        "$1+4+7=12 $\n",
        "<br>\n",
        "$2+5+8=15 $\n",
        "<br>\n",
        "$3+6+9=18 $\n",
        "<br><br>\n",
        "\n",
        "In the case of \"matrix_sr_rows\", we do each row (each list in the list of lists):\n",
        "<br><br>\n",
        "$1+2+3=6 $\n",
        "<br>\n",
        "$4+5+6=15 $\n",
        "<br>\n",
        "$7+8+9=24 $"
      ],
      "metadata": {
        "id": "HMJOLkla2V1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identity Matrices and Diagonal Matrices\n",
        "An identity matrix is a matrix that if multiplied by another will return that matrix. Its the equivalent of multiplication of 1 if we are dealing with scalars/single values. I.e. $x \\times 1 = x$ irrespective of what value $x$ takes. In practice this means a matrix filled with zeros except for ones on the diagonal. As an example:"
      ],
      "metadata": {
        "id": "eadwLFQG3ux6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identity Matrix (using PyTorch)\n",
        "identity_matrix = torch.eye(3, dtype=torch.int32)\n",
        "identity_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waReLE4frNhy",
        "outputId": "1e470bf7-a825-4b25-cfc3-aadba4363143"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0],\n",
              "        [0, 1, 0],\n",
              "        [0, 0, 1]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's confirm this is indeed an indentity matrix:"
      ],
      "metadata": {
        "id": "G0bCR4wB7X2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identity Matrix\n",
        "test_matrix = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.int32)\n",
        "output_matrix = torch.matmul(test_matrix, identity_matrix)\n",
        "output_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Dwl1iYmrt4p",
        "outputId": "d16db979-5a52-4cf2-b9d1-fbfa397c6126"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6],\n",
              "        [7, 8, 9]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An identity matrix is a special case of diagonal matrices, which come up regularly in other settings as well. A diagonal matrix is any matrix that is all zeros except for on its diagonal (in an identity matrix recall the diagonal is filled with ones). As example:"
      ],
      "metadata": {
        "id": "GqXd-qK-8m6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example diagonal matrix in PyTorch\n",
        "diagonal = torch.tensor([1, 2, 3, 4])\n",
        "diagonal_matrix = torch.diag(diagonal)\n",
        "diagonal_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pdbgWgzsaGa",
        "outputId": "500cc753-d125-44e7-a319-55894a540ef7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0, 0],\n",
              "        [0, 2, 0, 0],\n",
              "        [0, 0, 3, 0],\n",
              "        [0, 0, 0, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can calculate the sum of the diagonal use TF's _trace_ function:"
      ],
      "metadata": {
        "id": "9POyFtDSMgwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example diagonal matrix in PyTorch (trace)\n",
        "diagonal_matrix_trace = torch.trace(diagonal_matrix)\n",
        "diagonal_matrix_trace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhMb1VrOslGk",
        "outputId": "fed1268a-0e73-4f08-b6ec-1fb5c792f60e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick math check:\n",
        "<br><br>\n",
        "$1+2+3+4=10$"
      ],
      "metadata": {
        "id": "AumEPOWlNNyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inverse Matrices\n",
        "An inverse matrix is a pair of matrices (let say $a$ and $b$) where multiplying the first ($a$) by its inverse matrix ($b$) results in an identity matrix. Let's see this in action:"
      ],
      "metadata": {
        "id": "feMKoHEOBIvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example matrices\n",
        "a_matrix = torch.tensor([[1, 2, 1], [4, 4, 5], [6, 7, 7]], dtype=torch.float32)\n",
        "b_matrix = torch.tensor([[-7, -7, 6], [2, 1, -1], [4, 5, -4]], dtype=torch.float32)\n",
        "\n",
        "# Calculate matrix products\n",
        "inverse_it = torch.matmul(a_matrix, b_matrix)\n",
        "inverse_it_again = torch.matmul(b_matrix, a_matrix)\n",
        "\n",
        "print(\"A matrix\")\n",
        "print(a_matrix)\n",
        "print(\"\\n\")\n",
        "print(\"B matrix\")\n",
        "print(b_matrix)\n",
        "print(\"\\n\")\n",
        "print(\"Inverse a -> b\")\n",
        "print(inverse_it)\n",
        "print(\"\\n\")\n",
        "print(\"Inverse b -> a\")\n",
        "print(inverse_it_again)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Calculating the inverse of a matrix using torch.inverse()\n",
        "a_inverse = torch.inverse(a_matrix)\n",
        "print(\"Inverse of A:\")\n",
        "print(a_inverse)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Verify the inverse by multiplying the matrix by its inverse\n",
        "identity_matrix = torch.matmul(a_matrix, a_inverse)\n",
        "print(\"A * A_inverse (should be close to identity matrix):\")\n",
        "identity_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUYHNnA_s39h",
        "outputId": "26882b9c-c64a-4632-db84-41bf5e784f57"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A matrix\n",
            "tensor([[1., 2., 1.],\n",
            "        [4., 4., 5.],\n",
            "        [6., 7., 7.]])\n",
            "\n",
            "\n",
            "B matrix\n",
            "tensor([[-7., -7.,  6.],\n",
            "        [ 2.,  1., -1.],\n",
            "        [ 4.,  5., -4.]])\n",
            "\n",
            "\n",
            "Inverse a -> b\n",
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n",
            "\n",
            "\n",
            "Inverse b -> a\n",
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n",
            "\n",
            "\n",
            "Inverse of A:\n",
            "tensor([[-7.0000, -7.0000,  6.0000],\n",
            "        [ 2.0000,  1.0000, -1.0000],\n",
            "        [ 4.0000,  5.0000, -4.0000]])\n",
            "\n",
            "\n",
            "A * A_inverse (should be close to identity matrix):\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "        [ 0.0000e+00,  1.0000e+00, -1.9073e-06],\n",
              "        [ 0.0000e+00,  0.0000e+00,  1.0000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transpose and Orthogonal Matrices\n",
        "We are familiar with the idea of transposing from our work with _pandas_ DataFrames. Basically a transpose flips a matrix so the columns become rows and vice versa. Let's visualise this:"
      ],
      "metadata": {
        "id": "5_EdeGBP-fmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example matrix\n",
        "pre_transpose_matrix = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "print(\"Original\")\n",
        "print(pre_transpose_matrix)\n",
        "\n",
        "# Transpose the matrix\n",
        "transpose_matrix = torch.transpose(pre_transpose_matrix, 0, 1) # or pre_transpose_matrix.T\n",
        "print(\"\\n\")\n",
        "print(\"Transposed!\")\n",
        "transpose_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2l_HE9utXnx",
        "outputId": "e2899523-c5d5-47a0-a9c8-79e631822609"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "\n",
            "\n",
            "Transposed!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 4, 7],\n",
              "        [2, 5, 8],\n",
              "        [3, 6, 9]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An orthogonal matrix is any matrix which remains the same when transposed. As an example:"
      ],
      "metadata": {
        "id": "8donXrW5-fuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example matrix\n",
        "pre_transpose_matrix_two = torch.tensor([[1, 2, 3], [2, 0, 2], [3, 2, 1]])\n",
        "print(\"Original\")\n",
        "print(pre_transpose_matrix_two)\n",
        "\n",
        "# Transpose the matrix\n",
        "transpose_matrix_two = torch.transpose(pre_transpose_matrix_two, 0, 1) # or pre_transpose_matrix_two.T\n",
        "print(\"\\n\")\n",
        "print(\"Transposed!\")\n",
        "transpose_matrix_two"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut7xWJ_IttgZ",
        "outputId": "fdf5525d-629e-4c1e-8000-5e1fc7d86c3b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original\n",
            "tensor([[1, 2, 3],\n",
            "        [2, 0, 2],\n",
            "        [3, 2, 1]])\n",
            "\n",
            "\n",
            "Transposed!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [2, 0, 2],\n",
              "        [3, 2, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Argmax Operations\n",
        "Argmax means to find the maximum value in a set of potential values. This could be the posterior distribution of a Bayesian function or the output of a function(s) applied to a specific dataset. E.g. we may have a function $y = 10x^2 - 2x^3$ (where $x$ is a positive integer). The maximum value of $y$ increases while $x$ is less than nine, but decrease from nine onwards (check it out in Excel!). Therefore, argmax will tells us that we achieve the maximum value of $y$ when $x=8$.\n",
        "\n",
        "In terms of vectors and matrices, we can use argmax to find the location of the maximum value:"
      ],
      "metadata": {
        "id": "r74kf-DrC6xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Argmax of a vector\")\n",
        "another_vector = torch.tensor([4, 12, 42, 5])\n",
        "vector_arg_max = torch.argmax(another_vector)\n",
        "print(vector_arg_max)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Argmax of a matrix\")\n",
        "another_matrix = torch.tensor([[1, 12, 5], [6, 5, 42]])\n",
        "matrix_arg_max = torch.argmax(another_matrix, dim=0)\n",
        "print(matrix_arg_max)\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt_qTuSNuRr8",
        "outputId": "932f2543-26b1-4b18-af21-872c931f02e3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Argmax of a vector\n",
            "tensor(2)\n",
            "\n",
            "\n",
            "Argmax of a matrix\n",
            "tensor([1, 0, 1])\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the first case (\"vector_arg_max\") our algorithm finds the maximum value as 42 and returns the index of this item (remember we count from 0 ... so therefore it is 2).\n",
        "\n",
        "In the second case we compare both rows and return the index of which is highest (0 if the top row and 1 if the bottom). In other words, our output is [\"bottom row\", \"top row\", \"bottom row\"]."
      ],
      "metadata": {
        "id": "A2PV-VpPLl0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Derivatives\n",
        "The essence of a derivative (a key concept of differential calculus) is to evaluate a function at some given point, and calculate the current rate of change. In a purely linear function the rate of change will be the same at any point ... i.e. in the function $y = \\beta x$ the rate of change associated with $x$ is $\\beta$ at any point. However, in a non-linear function we need to work a bit harder to get this rate of change.\n",
        "\n",
        "More formally we want to know the rate of change in $y$ (written as $\\Delta y$) as a ratio to the rate of change in $x$ (again ... $\\Delta x$). Fortunately there are lots of rules (it is mathematics after all) to calculate this.\n"
      ],
      "metadata": {
        "id": "vJkP2YspMWmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Power Rule\n",
        "One key concept you have likely seen in a calculus class somewhere is the _power rule_ for calculating the derivative of a function applied to a single variable (e.g. $x$). The power rule states:<br><br>\n",
        "$f(x) = x^n \\rightarrow f'(x) = nx^{n-1}$\n",
        "<br>(i.e. we calculate the derivative ($f'$) of the function ($x^n$) by calculating $ nx^{n-1}$).\n",
        "\n",
        "Let's see an example using the Python package _sympy_ for pretty outputs (inspired by Dario Radečić's [Medium post](https://towardsdatascience.com/taking-derivatives-in-python-d6229ba72c64)):"
      ],
      "metadata": {
        "id": "7DnO0-KJ5N0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = sym.Symbol('x')\n",
        "\n",
        "# differentiate a function that is x^n and n=4 ... i.e. the function is x^4\n",
        "sym.diff(x**4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "tKcTI3kNNxSn",
        "outputId": "1d29b5a4-03e4-49e0-a71d-9c7095365479"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4*x**3"
            ],
            "text/latex": "$\\displaystyle 4 x^{3}$"
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python tells us the answer is $4x^3$ but let's be sure by doing the math:\n",
        "* $ f(x) = x^4 $\n",
        "* $ f'(x) = 4x^{4-1} = 4x^{3}$\n",
        "\n",
        "Well done Python. Sorry I doubted you."
      ],
      "metadata": {
        "id": "2L0KK14q4aWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Product Rule\n",
        "The product rule applies when we want to calculate the product (multiplication) of two functions. For example we may the following function:<br><br>\n",
        "$ F(x) = f(x) \\times g(x) $\n",
        "<br><br>\n",
        "In such cases we can use the product rule defined as:<br><br>\n",
        "$ F(x) = f(x) \\times g'(x) + f'(x) \\times g(x) $<br><br>\n",
        "I.e. we multiply each function with the derivate of the other and add these together.\n",
        "\n",
        "We can see it in action. Given:<br><br>\n",
        "$ F(x) = f(x) \\times g(x) $<br>\n",
        "$ f(x) = x^3 $<br>\n",
        "$ g(x) = x^5 $\n",
        "<br><br>\n",
        "We can calculate the derivative of each as above:<br><br>\n",
        "$ f'(x) = 3x^{2-1} = 3x^{1} = 3x$ <br>\n",
        "$ g'(x) = 5x^{5-1} = 5x^{4} $\n",
        "<br><br>\n",
        "We then need to multiply each together and add them:<br><br>\n",
        "$ F'(x) = f(x) \\times g'(x) + f'(x) \\times g(x) $<br>\n",
        "$ F'(x) = x^3 \\times 5x^{4} + 3x \\times x^5  = 5x^7 + 3x^5 = 8x^7$\n",
        "<br><br>\n",
        "Let's verify this in sympy:\n",
        "\n"
      ],
      "metadata": {
        "id": "xSEVbY6_5cIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sym.diff(x**3 * x**5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "jqHS1yMV4Po7",
        "outputId": "3bf1526c-1ff0-4df9-a3d3-84e157e46eac"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8*x**7"
            ],
            "text/latex": "$\\displaystyle 8 x^{7}$"
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Chain Rule\n",
        "So far we've seen single functions (via the power rule) and multiplicative functions (via the product rule) ... now we will look at functions inside functions (i.e. nested functions) via the _chain rule_. The chain rule gets super-relevant to deep learning as ultimately the very nature of having multiple hidden layers means we have functions inside functions.\n",
        "\n",
        "Consider the following function:<br><br>\n",
        "$ F(x) = (x^3 - 2x + 4)^3 $\n",
        "<br><br>\n",
        "We have an ($x^3 - 2x + 4)$ as an inner function and an outer function that raises the inner function to the power 3. The chain rule says:<br><br>\n",
        "$ F(x) = f(g(x)) \\rightarrow F'(x) = f'(g(x)) \\times g'(x)$\n",
        "<br><br>\n",
        "In other words we reach the overall derivative by taking the derivative of the outer function multiplied by the inner function (kind of like the first half of the product rule), multiplied by the derivative of the inner function. It is almost certainly clearer if we look at the math on our earlier example function.<br><br>\n",
        "$ F(x) = (x^3 - 2x + 4)^3 $<br><br>\n",
        "$ F'(x) = 3(x^3 - 2x + 4)^{3-1} \\times 3x^2 - 2x^{1-1}$<br>\n",
        "_A note on the calculation of the inner function here. We can consider $2x$ as effectively $2x^1$ in terms of doing our power rule calculations. This means we end at the power $1-1$ and the $x$ will be cancelled out. We also ignore constants_ ($+4$) _when calculating a derivative. Note over, let's do some simplifications_ <br><br>\n",
        "$F'(x) = 3(x^3 - 2x + 4)^{2} \\times 3x^2 - 2$\n",
        "<br><br>\n",
        "$F'(x) = (9x^2 -6) \\times (x^3 - 2x + 4)^{2} $\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "EuEIV0C55ssm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sym.diff((x**3 - 2 * x + 4)**3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 41
        },
        "id": "w50UHidi5usA",
        "outputId": "8c327372-1b0a-4e84-edba-c1d5cd7f8230"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9*x**2 - 6)*(x**3 - 2*x + 4)**2"
            ],
            "text/latex": "$\\displaystyle \\left(9 x^{2} - 6\\right) \\left(x^{3} - 2 x + 4\\right)^{2}$"
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partial Derivatives\n",
        "The chain rule gets us a big chunk of the way towards what we need in terms of using derivatives to understand parameter optimisation in deep learning (which will be explained in the module don't worry), except for one thing. Everything we've looked at so far has looked at changes in $y$ with respect to a single $x$. We theoretically could have just one feature, but in ML/AI practice that is extremely unlikely. If we have multiple features ($x$'s), which we will, we need _partial derivatives_.\n",
        "\n",
        "Partial derivatives deal with multi-variable functions by applying the usual rules we've just seen to a single variable in the function and effectively freezing the others (keeping them constant). We can follow this process for each of the variable in the function.\n",
        "\n",
        "Again let's make up a function to work with:<br><br>\n",
        "$ f(x_{1}, x_{2}, x_{3}) = {x_{1}}^2 \\times x_{2} \\times {x_{3}}^4 $\n",
        "<br><br>\n",
        "As above, the derivatives we seek will be partial ... i.e. we will find the derivative of $x_1$ independently of the other $x$'s. In our earlier discussion we were finding the ration between $\\Delta y$ and $\\Delta x$. Given this is a subset of the overall problem we use the lower case version of delta to show the partial ... so the notation would be $\\delta_{x_{1}}$ (double subscripts is a bit yucky in LaTex ... sorry). Let's look at the partial derivative of $X_{3}$:\n",
        "<br><br>\n",
        "$ \\delta_{x_{3}} = 4 \\times ({x_{1}}^2 \\times x_{2} \\times {x_{3}}^3)$\n",
        "<br><br>\n",
        "Essentially we are doing normal power rule stuff here. ${x_{3}}^4$ becomes $ 4 \\times {x_{3}}^3 $. The only difference is we keep the rest of the formula in and as-is. For completion, we can write out all three partial derivatives (but without discussion of the calculations - its the same as we've already seen):<br><br>\n",
        "$ f(x_{1}, x_{2}, x_{3}) = {x_{1}}^2 \\times x_{2} \\times {x_{3}}^4 $\n",
        "<br>\n",
        "$ \\delta_{x_{1}} = 2 \\times x_{1} \\times x_{2} \\times {x_{3}}^4 $\n",
        "<br>\n",
        "$ \\delta_{x_{2}} = {x_{1}}^2 \\times {x_{3}}^4$\n",
        "<br>\n",
        "$ \\delta_{x_{3}} = 4 \\times ({x_{1}}^2 \\times x_{2} \\times {x_{3}}^3)$\n",
        "<br><br>\n",
        "Let's verify in sympy, and to make things easier we'll also write out the function rather than typing it each time:\n"
      ],
      "metadata": {
        "id": "ZZy5aonS5vED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1, x2, x3 = sym.symbols('x1 x2 x3')\n",
        "f = x1**2 * x2 * x3**4\n",
        "\n",
        "print(\"Delta for x1\")\n",
        "print(sym.diff(f, x1))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Delta for x2\")\n",
        "print(sym.diff(f, x2))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Delta for x3\")\n",
        "print(sym.diff(f, x3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkCsst9f52Ht",
        "outputId": "f3034c77-2d68-4d93-b7d4-9fe15386af93"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Delta for x1\n",
            "2*x1*x2*x3**4\n",
            "\n",
            "\n",
            "Delta for x2\n",
            "x1**2*x3**4\n",
            "\n",
            "\n",
            "Delta for x3\n",
            "4*x1**2*x2*x3**3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything checks out! And the good news is now the math is over - we will touch on how these concepts fit into the deep learning process in class ... but these aren't calculations we'll have to make ourselves. However, it is useful to get some understanding on what's happening under the tin of tools like Pytorch and TensorFlow."
      ],
      "metadata": {
        "id": "m56FRM2O52cH"
      }
    }
  ]
}