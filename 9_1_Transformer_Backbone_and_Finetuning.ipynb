{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOIBBUmPJNO8hYwk4J6+6b0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJMortensonWarwick/ADA2425/blob/main/9_1_Transformer_Backbone_and_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers: Backbone and Fine-tuning\n",
        "In the lecture we have seen the overall architecture and approach of transformers. We have also seen that in general transformers have allowed us to build very large-scale models of billions of paramaters (maybe even trillions).\n",
        "\n",
        "How should we interact with such models? It is obviously difficult for us to train our own transformer of this scale, and if we train a much smaller model, can it compete?\n",
        "\n",
        "Instead, we may want to build on top of a transfomer trained at web-scale. This Notebook will explore a couple of methods of doing this.\n",
        "\n",
        "First, let's install some packages and some data:"
      ],
      "metadata": {
        "id": "qYCPLrjSlzCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load Dataset and Tokeniser\n",
        "dataset = load_dataset(\"imdb\") # use the in-built IMDB review dataset"
      ],
      "metadata": {
        "id": "DkSL-vczc4eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used some standard libraries, most of which we have seen. We have also installed the IMDB dataset - a the name suggests a set of movie details and reviews.\n",
        "\n",
        "We will now start to use some transformer technology! We need to convert our data first of all to embeddings. We'll do so with the popular [BERT](https://arxiv.org/abs/1810.04805) model from Google:"
      ],
      "metadata": {
        "id": "INu0ErOsmuYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # we want the data to be the same size each time but some reviews are shorter\n",
        "    # if any are less than max_length, we will add zeros to the end of it until\n",
        "    # it is the same length as max_length (padding=\"max_length\")\n",
        "    # if any are longer than max_length (could happen in production), then\n",
        "    # we truncate it - i.e. delete any characters after max_length is reached.\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# tokenise the data (covert to embeddings)\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "1ChUV7uNdAt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have tokenised our data using BERT ... i.e. converted the text documents to a vector format where each document is now a set of vectors that represents the semantic meaning of the text.\n",
        "\n",
        "We can use this feature extracted dataset in a variety of ways, two of which we will explore. The first is taking this as basically a feature engineered dataset, and training a simple prediction \"head\" on top of this. I.e. we will not attempt to train the transformer model at all - we will use it with the weights learned through the training regime Google used.\n",
        "\n",
        "Instead we will just use that as our backbone, with a simple, dense neural network on top that we will train. First, let's build that as a model:"
      ],
      "metadata": {
        "id": "SiQ1EBwtnYrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Embeddings-Only Model with Custom Head\n",
        "\n",
        "class CustomHeadModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_labels):\n",
        "        super(CustomHeadModel, self).__init__()\n",
        "        self.embedding_dim = embedding_dim # input\n",
        "        self.linear1 = nn.Linear(embedding_dim, 64) # linear layer\n",
        "        self.relu = nn.ReLU() # ReLU activation\n",
        "        self.linear2 = nn.Linear(64, num_labels) # linear layer - binary output\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        x = self.linear1(embeddings)\n",
        "        x = self.relu(x)\n",
        "        return self.linear2(x)\n",
        "\n",
        "# create a model from the pretrained embeddings\n",
        "model_embeddings = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# get the size of the embeddings (number of dimensions)\n",
        "embedding_dim = model_embeddings.config.hidden_size\n",
        "\n",
        "num_labels = 2 # Binary classification (positive: 0, negative: 1)\n",
        "\n",
        "# setup the custom head to take embeddings size as input and labels as output\n",
        "# note this time we are doing binary classification with two neurons to keep\n",
        "# num_labels as flexible and something we could change to a higher number of labels\n",
        "# this works the same way as output=1. Its slightly less efficient but fine\n",
        "custom_head = CustomHeadModel(embedding_dim, num_labels)"
      ],
      "metadata": {
        "id": "uGNE_vp-fakK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have a similar codebase to the one we have seen before. The difference is essentialy at the input layer. Rather than feeding in raw data, we have fed in the embeddings from BERT.\n",
        "\n",
        "In our model the input layer is based on the shape of the embeddings we create with BERT and incorporate into our architecture via [AutoModel](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html). We then add a two layer DNN with ReLU and 64 neurons in the first layer and then we build our output layer as 2x neurons \"negative\" review and \"positive\".\n",
        "\n",
        "Note, we have previously used only one neuron for binary classification, which is absolutely fine to do here. Two neurons without softmax achieves the same thing - somewhat less efficiently - as we can just pick whichever outputs the highest number (if the first neuron we say the review is negative; if the second neuron we say positive)."
      ],
      "metadata": {
        "id": "LliGIjE0ol6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings-only prediction (with training of the classification head)\n",
        "\n",
        "# Freeze the embeddings\n",
        "for param in model_embeddings.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_embeddings.to(device) # add embedding model to GPU\n",
        "custom_head.to(device) # add custom head to GPU\n",
        "\n",
        "optimizer = optim.AdamW(custom_head.parameters(), lr=5e-5, weight_decay=0.0005) # Only optimise the custom head\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# use a subset of 1,000 random records for demonstration\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle().select(range(1000))\n",
        "\n",
        "# Data is a list including:\n",
        "# 1. id of words\n",
        "# 2. attention mask\n",
        "# 3. label\n",
        "\n",
        "# Custom collate function to handle lists which DataLoader doesn't do automatically\n",
        "def custom_collate(batch):\n",
        "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.tensor([item['attention_mask'] for item in batch])\n",
        "    labels = torch.tensor([item['label'] for item in batch])\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': labels}\n",
        "\n",
        "\n",
        "train_loader = DataLoader(small_train_dataset, shuffle=True, batch_size=16, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "dtMB0dO8dNd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have the model we can set up for training. At the very start of the code, we have made the embeddings part of the model non-trainable (by setting these parameters as \"requires\\_grad = False\"). This means during training only the weights in the custom head will be updated by backprop.\n",
        "\n",
        "We specify a loss function (note this is now CrossEntropy rather than BinaryCrossEntropy becuase we are using two neurons).\n",
        "\n",
        "We create a DataLoader as before. The only difference is we have a customer collate function because our embedding output will effectively be alist each time including id's, attention masks and labels. In the code above this, we load in a subset of the data (1,000 rows) to reduce training time."
      ],
      "metadata": {
        "id": "Z8_b6UfQqEQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "custom_head.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        epoch_loss = 0 # reset loss\n",
        "\n",
        "        # add data to GPU\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        with torch.no_grad(): # Freeze the bert/embeddings model\n",
        "            embeddings = model_embeddings(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = custom_head(embeddings) # get logits for custom head\n",
        "        loss = criterion(logits, labels) # get loss for custom head\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {round(epoch_loss/len(train_loader), 4)}\")"
      ],
      "metadata": {
        "id": "1dD855X0dfgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training is a before. We can see from the very epoch we can see a low level of loss, and it stays about the same through each of the epochs. What does this tell you?\n",
        "\n",
        "Now we can make predictions:"
      ],
      "metadata": {
        "id": "7YUTaKdfrbhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"This is a great movie!\"\n",
        "encoded_input = tokenizer(example_text, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    embeddings = model_embeddings(**encoded_input).pooler_output\n",
        "    logits = custom_head(embeddings)\n",
        "\n",
        "    # select the highest probability neuron as the prediction\n",
        "    predictions = torch.argmax(logits, dim=1) # dim 0 is neurons, dim 1 is probability of each\n",
        "\n",
        "    # print result\n",
        "    print_label = \"positive\" if predictions.item() == 0 else \"negative\"\n",
        "    print(f\"Prediction for '{example_text}': {print_label}\")"
      ],
      "metadata": {
        "id": "e6170GPMeuUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We predict a very basic example \"This is a great movie!\". Unsuprisingly, given the low level of loss, this is easy for our model and it predicts it correctly.\n",
        "\n",
        "Now let's try another approach - fine-tuning. In this we will train the whole model:"
      ],
      "metadata": {
        "id": "gV2_DyTIrs6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning the Model with the Custom Head\n",
        "\n",
        "# Reset the custom head so it is untrained\n",
        "custom_head = CustomHeadModel(embedding_dim, num_labels)\n",
        "\n",
        "# combine the embeddings model and custom head into one model\n",
        "model_fine_tune = nn.Sequential(model_embeddings,custom_head)\n",
        "model_fine_tune.to(device) # add the combined model to GPU\n",
        "\n",
        "# this time optimise all parameters\n",
        "optimizer = optim.AdamW(model_fine_tune.parameters(), lr=5e-5, weight_decay=0.0005)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# use a subset of 1,000 records for demonstration\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle().select(range(1000))\n",
        "\n",
        "# Custom collate function to handle lists which DataLoader doesn't do automatically\n",
        "def custom_collate(batch):\n",
        "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.tensor([item['attention_mask'] for item in batch])\n",
        "    labels = torch.tensor([item['label'] for item in batch])\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': labels}\n",
        "\n",
        "train_loader = DataLoader(small_train_dataset, shuffle=True, batch_size=16, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "wYXqpjvvfCxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have reset the model - reverted the weights of the custom head back to zero (actually random, but the same meaning).\n",
        "\n",
        "Now we build a new model, \"model\\_fine\\_tune\", with our embeddings and our reset, custom head. Note, we do not set any part of the model to \"frozen\" (i.e. \"requires\\_grad = False\") ... we will be training all of it.\n",
        "\n",
        "The last parts of the code just rebuild the DataLoader. If you are running the whole Notebook you don't actually need to do this, I've included it just in case you want to copy out for your own project.\n",
        "\n",
        "Now we can train as before:"
      ],
      "metadata": {
        "id": "M3J4V05lsB-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_fine_tune.train()\n",
        "for epoch in range(num_epochs): #train for 3 epochs\n",
        "  for batch in train_loader:\n",
        "    epoch_loss = 0\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = torch.tensor(batch['label']).to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Pass input_ids and attention_mask to the first module in the sequence (model_embeddings)\n",
        "    # and then pass its output to the next module (custom_head)\n",
        "    outputs = model_fine_tune[1](model_fine_tune[0](input_ids=input_ids, attention_mask=attention_mask).pooler_output)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {round(epoch_loss/len(train_loader), 4)}\")"
      ],
      "metadata": {
        "id": "SU7JwktfiYzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything here is code as we've previously seen. We also see the training outcomes which ... are about the same as before (although with a much slower training process as we are also training the embedding model). We can also try a prediction:"
      ],
      "metadata": {
        "id": "EN7soBjTtC_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"This is a terrible movie!\"\n",
        "encoded_input = tokenizer(example_text, return_tensors=\"pt\").to(device) # covert example to tokens\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Pass input_ids and attention_mask to the first module in the sequence (model_embeddings)\n",
        "    # and then pass its output to the next module (custom_head)\n",
        "    # **encoded_input means unpack the list of items\n",
        "    outputs = model_fine_tune[1](model_fine_tune[0](**encoded_input).pooler_output) # Pass encoded_input as keyword arguments\n",
        "\n",
        "    # select the highest probability neuron as the prediction\n",
        "    predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # print result\n",
        "    print_label = \"positive\" if predictions.item() == 0 else \"negative\"\n",
        "    print(f\"Prediction for '{example_text}': {print_label}\")"
      ],
      "metadata": {
        "id": "qcRH_XIfimgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not a great prediction. However, what should we conclude?"
      ],
      "metadata": {
        "id": "GxM9D_YSuhrZ"
      }
    }
  ]
}